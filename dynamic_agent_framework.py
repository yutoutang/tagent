"""
åŠ¨æ€ Agent è°ƒç”¨æ¡†æ¶
åŸºäº LangGraph å®ç°ä»»åŠ¡è¯†åˆ«ã€åŠ¨æ€å·¥å…·åŠ è½½å’Œå¤š agent åä½œ
"""

import os
import json
from typing import Any, Dict, List, Optional, TypedDict, Annotated
from enum import Enum
from operator import add
from pathlib import Path

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from pydantic import BaseModel, Field


# ============================================================================
# 1. ä»»åŠ¡åˆ†ç±»ä¸çŠ¶æ€å®šä¹‰
# ============================================================================

class TaskType(str, Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    CODING = "coding"
    RESEARCH = "research"
    ANALYSIS = "analysis"
    CALCULATION = "calculation"
    GENERAL = "general"


class AgentState(TypedDict):
    """Agent å…±äº«çŠ¶æ€"""
    # æ¶ˆæ¯å†å²ï¼ˆä½¿ç”¨ add_messages reducer è‡ªåŠ¨è¿½åŠ ï¼‰
    messages: Annotated[List[BaseMessage], add_messages]

    # ä»»åŠ¡è¯†åˆ«
    task_type: Optional[str]
    task_confidence: Optional[float]

    # åŠ¨æ€å·¥å…·
    available_tools: List[Dict[str, Any]]
    executed_tools: List[str]

    # æ‰§è¡Œç»“æœ
    result: Optional[str]
    intermediate_steps: Annotated[List[Dict], add]

    # æ§åˆ¶æµ
    iteration: int
    max_iterations: int
    is_complete: bool

    # é”™è¯¯å¤„ç†
    errors: Annotated[List[str], add]

    # å…ƒæ•°æ®
    metadata: Dict[str, Any]


class TaskClassification(BaseModel):
    """ä»»åŠ¡åˆ†ç±»ç»“æœ"""
    task_type: TaskType = Field(description="ä»»åŠ¡ç±»å‹")
    confidence: float = Field(description="åˆ†ç±»ç½®ä¿¡åº¦ 0-1")
    required_tools: List[str] = Field(description="éœ€è¦çš„å·¥å…·åˆ—è¡¨")
    reasoning: str = Field(description="åˆ†ç±»ç†ç”±")


# ============================================================================
# 2. å·¥å…·æ³¨å†Œç³»ç»Ÿ
# ============================================================================

class ToolRegistry:
    """åŠ¨æ€å·¥å…·æ³¨å†Œè¡¨"""

    def __init__(self):
        self._tools: Dict[str, Any] = {}
        self._tool_metadata: Dict[str, Dict] = {}

    def register(self, name: str, tool_func: Any, metadata: Optional[Dict] = None):
        """æ³¨å†Œå·¥å…·"""
        self._tools[name] = tool_func
        self._tool_metadata[name] = metadata or {}

    def get(self, name: str) -> Optional[Any]:
        """è·å–å·¥å…·"""
        return self._tools.get(name)

    def get_by_task_type(self, task_type: str) -> List[Any]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–ç›¸å…³å·¥å…·"""
        return [
            self._tools[name]
            for name, meta in self._tool_metadata.items()
            if task_type.lower() in meta.get("task_types", [])
        ]

    def list_tools(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å·¥å…·åŠå…¶å…ƒæ•°æ®"""
        return [
            {"name": name, **meta}
            for name, meta in self._tool_metadata.items()
        ]

    def load_tools_from_config(self, config_path: str):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½å·¥å…·å®šä¹‰"""
        config = json.loads(Path(config_path).read_text())
        for tool_config in config.get("tools", []):
            # è¿™é‡Œå¯ä»¥åŠ¨æ€åŠ è½½å·¥å…·æ¨¡å—
            pass


# å…¨å±€å·¥å…·æ³¨å†Œè¡¨
tool_registry = ToolRegistry()


# ============================================================================
# 3. å†…ç½®å·¥å…·å®šä¹‰
# ============================================================================

@tool
def code_analyzer(code: str, language: str = "python") -> str:
    """åˆ†æä»£ç å¹¶æä¾›å»ºè®®"""
    return f"åˆ†æäº† {language} ä»£ç ï¼Œå‘ç° {len(code.splitlines())} è¡Œä»£ç "


@tool
def web_searcher(query: str, max_results: int = 5) -> str:
    """æœç´¢ç½‘ç»œä¿¡æ¯"""
    return f"æ‰¾åˆ° {max_results} ä¸ªå…³äº '{query}' çš„ç»“æœ"


@tool
def data_calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        result = eval(expression, {"__builtins__": {}}, {})
        return f"è®¡ç®—ç»“æœ: {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"


@tool
def document_summarizer(text: str) -> str:
    """æ€»ç»“æ–‡æ¡£å†…å®¹"""
    return f"æ–‡æ¡£æ€»ç»“ï¼ˆå…± {len(text)} ä¸ªå­—ç¬¦ï¼‰: {text[:100]}..."


@tool
def api_client(endpoint: str, method: str = "GET") -> str:
    """è°ƒç”¨ API æ¥å£"""
    return f"è°ƒç”¨ API: {method} {endpoint}"


# æ³¨å†Œå†…ç½®å·¥å…·
tool_registry.register("code_analyzer", code_analyzer, {
    "task_types": ["coding", "analysis"],
    "description": "ä»£ç åˆ†æä¸å»ºè®®"
})

tool_registry.register("web_searcher", web_searcher, {
    "task_types": ["research", "general"],
    "description": "ç½‘ç»œæœç´¢"
})

tool_registry.register("data_calculator", data_calculator, {
    "task_types": ["calculation", "analysis"],
    "description": "æ•°å­¦è®¡ç®—"
})

tool_registry.register("document_summarizer", document_summarizer, {
    "task_types": ["research", "analysis"],
    "description": "æ–‡æ¡£æ€»ç»“"
})

tool_registry.register("api_client", api_client, {
    "task_types": ["coding", "general"],
    "description": "API è°ƒç”¨"
})


# ============================================================================
# 4. LLM åˆå§‹åŒ–
# ============================================================================

def create_llm():
    """åˆ›å»º LLM å®ä¾‹"""
    provider = os.getenv("LLM_PROVIDER", "openai")

    if provider == "anthropic":
        return ChatAnthropic(
            model=os.getenv("MODEL_NAME", "claude-3-5-sonnet-20241022"),
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
    else:
        return ChatOpenAI(
            model=os.getenv("MODEL_NAME", "gpt-4o"),
            api_key=os.getenv("OPENAI_API_KEY"),
            temperature=0
        )


# ============================================================================
# 5. Agent èŠ‚ç‚¹å®šä¹‰
# ============================================================================

def task_classifier_node(state: AgentState) -> Dict[str, Any]:
    """ä»»åŠ¡åˆ†ç±»èŠ‚ç‚¹ - è¯†åˆ«ä»»åŠ¡ç±»å‹å¹¶åŠ è½½ç›¸åº”å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1] if messages else None

    if not last_message:
        return {"task_type": TaskType.GENERAL, "task_confidence": 0.5}

    # ä½¿ç”¨ LLM è¿›è¡Œç»“æ„åŒ–ä»»åŠ¡åˆ†ç±»
    llm = create_llm()
    classifier = llm.with_structured_output(TaskClassification)

    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡åˆ†ç±»ä¸“å®¶ã€‚åˆ†æç”¨æˆ·è¯·æ±‚å¹¶åˆ†ç±»ä¸ºä»¥ä¸‹ç±»å‹ä¹‹ä¸€ï¼š
- coding: ç¼–ç¨‹ã€ä»£ç åˆ†æã€å¼€å‘ä»»åŠ¡
- research: ä¿¡æ¯æœç´¢ã€è°ƒç ”ã€æ–‡æ¡£å¤„ç†
- analysis: æ•°æ®åˆ†æã€æ¨ç†ã€è¯„ä¼°
- calculation: æ•°å­¦è®¡ç®—ã€æ•°å€¼å¤„ç†
- general: ä¸€èˆ¬å¯¹è¯ã€å…¶ä»–ä»»åŠ¡

åŒæ—¶è¯†åˆ«å®Œæˆä»»åŠ¡éœ€è¦çš„å·¥å…·ã€‚"""

    try:
        result = classifier.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=last_message.content)
        ])

        # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ è½½ç›¸åº”å·¥å…·
        relevant_tools = tool_registry.get_by_task_type(result.task_type)
        tools_info = [
            {"name": tool.name, "description": tool.description}
            for tool in relevant_tools
        ]

        return {
            "task_type": result.task_type,
            "task_confidence": result.confidence,
            "available_tools": tools_info,
            "intermediate_steps": [{
                "step": "classification",
                "result": result.dict()
            }]
        }

    except Exception as e:
        return {
            "task_type": TaskType.GENERAL,
            "task_confidence": 0.5,
            "errors": [f"åˆ†ç±»å¤±è´¥: {str(e)}"]
        }


def planner_node(state: AgentState) -> Dict[str, Any]:
    """è§„åˆ’èŠ‚ç‚¹ - åˆ¶å®šæ‰§è¡Œè®¡åˆ’"""
    task_type = state.get("task_type", TaskType.GENERAL)
    available_tools = state.get("available_tools", [])

    llm = create_llm()

    prompt = f"""ä»»åŠ¡ç±»å‹: {task_type}
å¯ç”¨å·¥å…·: {', '.join([t['name'] for t in available_tools])}

è¯·åˆ¶å®šä¸€ä¸ªæ‰§è¡Œè®¡åˆ’æ¥å®Œæˆç”¨æˆ·çš„ä»»åŠ¡ã€‚è€ƒè™‘ï¼š
1. éœ€è¦ä½¿ç”¨å“ªäº›å·¥å…·
2. æ‰§è¡Œçš„é¡ºåº
3. é¢„æœŸçš„ä¸­é—´æ­¥éª¤
"""

    try:
        response = llm.invoke([
            HumanMessage(content=prompt),
            *state["messages"][-3:]  # åŒ…å«æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡
        ])

        return {
            "messages": [response],
            "intermediate_steps": [{
                "step": "planning",
                "plan": response.content
            }]
        }

    except Exception as e:
        return {"errors": [f"è§„åˆ’å¤±è´¥: {str(e)}"]}


def executor_node(state: AgentState) -> Dict[str, Any]:
    """æ‰§è¡ŒèŠ‚ç‚¹ - æ‰§è¡Œå·¥å…·å¹¶å¤„ç†ç»“æœ"""
    task_type = state.get("task_type", TaskType.GENERAL)
    iteration = state.get("iteration", 0)

    # è·å–ç›¸å…³å·¥å…·
    relevant_tools = tool_registry.get_by_task_type(task_type)

    if not relevant_tools:
        return {
            "messages": [AIMessage(content="æ²¡æœ‰å¯ç”¨çš„ç›¸å…³å·¥å…·")],
            "is_complete": True
        }

    # åˆ›å»ºå¸¦å·¥å…·çš„ LLM
    llm = create_llm()
    llm_with_tools = llm.bind_tools(relevant_tools)

    try:
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        response = llm_with_tools.invoke(state["messages"])

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if response.tool_calls:
            executed_tools = []
            results = []

            for tool_call in response.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]

                # æ‰§è¡Œå·¥å…·
                tool_obj = tool_registry.get(tool_name)
                if tool_obj:
                    result = tool_obj.invoke(tool_args)
                    executed_tools.append(tool_name)
                    results.append(f"{tool_name}: {result}")

            return {
                "messages": [response, AIMessage(content="\n".join(results))],
                "executed_tools": executed_tools,
                "intermediate_steps": [{
                    "step": "execution",
                    "tools": executed_tools,
                    "iteration": iteration
                }]
            }
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä»»åŠ¡å®Œæˆ
            return {
                "messages": [response],
                "is_complete": True,
                "result": response.content
            }

    except Exception as e:
        return {
            "errors": [f"æ‰§è¡Œå¤±è´¥: {str(e)}"],
            "is_complete": True
        }


def reflector_node(state: AgentState) -> Dict[str, Any]:
    """åæ€èŠ‚ç‚¹ - è¯„ä¼°ç»“æœå¹¶å†³å®šæ˜¯å¦éœ€è¦ç»§ç»­"""
    messages = state["messages"]
    executed_tools = state.get("executed_tools", [])
    is_complete = state.get("is_complete", False)
    iteration = state.get("iteration", 0)

    if is_complete or iteration >= state.get("max_iterations", 5):
        return {"is_complete": True}

    # ä½¿ç”¨ LLM è¯„ä¼°æ˜¯å¦éœ€è¦ç»§ç»­
    llm = create_llm()

    prompt = f"""å·²æ‰§è¡Œçš„å·¥å…·: {', '.join(executed_tools)}
å½“å‰è¿­ä»£: {iteration}

è¯„ä¼°æ˜¯å¦éœ€è¦ç»§ç»­æ‰§è¡Œä»¥å®Œæˆä»»åŠ¡ã€‚å¦‚æœå·²ç»å¾—åˆ°æ»¡æ„çš„ç­”æ¡ˆï¼Œè¿”å› "complete"ã€‚
å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯æˆ–æ‰§è¡Œæ›´å¤šæ­¥éª¤ï¼Œè¿”å› "continue"ã€‚"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])

        should_continue = "continue" in response.content.lower()

        return {
            "is_complete": not should_continue,
            "iteration": iteration + 1,
            "intermediate_steps": [{
                "step": "reflection",
                "decision": "continue" if should_continue else "complete"
            }]
        }

    except Exception as e:
        return {
            "errors": [f"åæ€å¤±è´¥: {str(e)}"],
            "is_complete": True
        }


def synthesizer_node(state: AgentState) -> Dict[str, Any]:
    """ç»¼åˆèŠ‚ç‚¹ - ç”Ÿæˆæœ€ç»ˆå›ç­”"""
    messages = state["messages"]
    intermediate_steps = state.get("intermediate_steps", [])

    llm = create_llm()

    prompt = f"""åŸºäºä»¥ä¸‹æ‰§è¡Œæ­¥éª¤ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”ï¼š

æ‰§è¡Œæ­¥éª¤:
{json.dumps(intermediate_steps, indent=2, ensure_ascii=False)}

å¯¹è¯å†å²:
{[m.content for m in messages[-5:]]}

è¯·æä¾›æ¸…æ™°ã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚"""

    try:
        response = llm.invoke([HumanMessage(content=prompt)])

        return {
            "result": response.content,
            "messages": [response],
            "is_complete": True
        }

    except Exception as e:
        return {
            "errors": [f"ç»¼åˆå¤±è´¥: {str(e)}"],
            "result": f"å¤„ç†å®Œæˆï¼Œä½†ç»¼åˆæ­¥éª¤å‡ºé”™: {str(e)}"
        }


# ============================================================================
# 6. è·¯ç”±é€»è¾‘
# ============================================================================

def should_continue(state: AgentState) -> str:
    """å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œ"""
    if state.get("is_complete", False):
        return "synthesize"
    if state.get("iteration", 0) >= state.get("max_iterations", 5):
        return "synthesize"
    if state.get("errors"):
        return "synthesize"
    return "execute"


def route_after_planning(state: AgentState) -> str:
    """è§„åˆ’åè·¯ç”±å†³ç­–"""
    task_type = state.get("task_type", "")
    available_tools = state.get("available_tools", [])

    if not available_tools:
        # æ²¡æœ‰å¯ç”¨å·¥å…·ï¼Œç›´æ¥ç”Ÿæˆå›ç­”
        return "synthesize"

    # æœ‰å·¥å…·å¯ç”¨ï¼Œè¿›å…¥æ‰§è¡Œé˜¶æ®µ
    return "execute"


# ============================================================================
# 7. å›¾æ„å»º
# ============================================================================

def create_dynamic_agent_graph():
    """åˆ›å»ºåŠ¨æ€ agent è®¡ç®—å›¾"""

    # åˆå§‹åŒ–çŠ¶æ€å›¾
    graph = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("classify", task_classifier_node)
    graph.add_node("plan", planner_node)
    graph.add_node("execute", executor_node)
    graph.add_node("reflect", reflector_node)
    graph.add_node("synthesize", synthesizer_node)

    # æ·»åŠ è¾¹
    graph.add_edge(START, "classify")
    graph.add_edge("classify", "plan")
    graph.add_conditional_edges(
        "plan",
        route_after_planning,
        {
            "execute": "execute",
            "synthesize": "synthesize"
        }
    )
    graph.add_conditional_edges(
        "execute",
        should_continue,
        {
            "execute": "reflect",
            "synthesize": "synthesize"
        }
    )
    graph.add_conditional_edges(
        "reflect",
        should_continue,
        {
            "execute": "execute",
            "synthesize": "synthesize"
        }
    )
    graph.add_edge("synthesize", END)

    # ç¼–è¯‘å›¾ï¼ˆæ·»åŠ æŒä¹…åŒ–æ”¯æŒï¼‰
    memory = MemorySaver()
    app = graph.compile(checkpointer=memory)

    return app


# ============================================================================
# 8. Agent æ¥å£
# ============================================================================

class DynamicAgent:
    """åŠ¨æ€ Agent æ¥å£"""

    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ– agent"""
        self.app = create_dynamic_agent_graph()
        self.config = config or {
            "configurable": {"thread_id": "default_session"}
        }

    def run(self, message: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡Œ agent"""
        config = self.config.copy()
        if session_id:
            config["configurable"]["thread_id"] = session_id

        initial_state: AgentState = {
            "messages": [HumanMessage(content=message)],
            "task_type": None,
            "task_confidence": None,
            "available_tools": [],
            "executed_tools": [],
            "result": None,
            "intermediate_steps": [],
            "iteration": 0,
            "max_iterations": 5,
            "is_complete": False,
            "errors": [],
            "metadata": {}
        }

        try:
            result = self.app.invoke(initial_state, config)
            return {
                "success": True,
                "result": result.get("result"),
                "task_type": result.get("task_type"),
                "intermediate_steps": result.get("intermediate_steps"),
                "errors": result.get("errors", [])
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "errors": [str(e)]
            }

    async def astream(self, message: str, session_id: Optional[str] = None):
        """å¼‚æ­¥æµå¼è¿è¡Œ"""
        config = self.config.copy()
        if session_id:
            config["configurable"]["thread_id"] = session_id

        initial_state: AgentState = {
            "messages": [HumanMessage(content=message)],
            "task_type": None,
            "task_confidence": None,
            "available_tools": [],
            "executed_tools": [],
            "result": None,
            "intermediate_steps": [],
            "iteration": 0,
            "max_iterations": 5,
            "is_complete": False,
            "errors": [],
            "metadata": {}
        }

        async for event in self.app.astream(initial_state, config):
            yield event

    def chat(self, message: str, session_id: Optional[str] = None) -> str:
        """ç®€å•èŠå¤©æ¥å£"""
        result = self.run(message, session_id)
        if result["success"]:
            return result["result"] or "å¤„ç†å®Œæˆ"
        else:
            return f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"

    def register_tool(self, name: str, tool_func: Any, metadata: Optional[Dict] = None):
        """æ³¨å†Œæ–°å·¥å…·"""
        tool_registry.register(name, tool_func, metadata)

    def list_tools(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·"""
        return tool_registry.list_tools()


# ============================================================================
# 9. ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""

    # åŠ è½½ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv()

    # åˆ›å»º agent
    agent = DynamicAgent()

    print("=" * 60)
    print("åŠ¨æ€ Agent è°ƒç”¨æ¡†æ¶")
    print("=" * 60)

    # ç¤ºä¾‹ 1: ç¼–ç¨‹ä»»åŠ¡
    print("\nğŸ“ ç¤ºä¾‹ 1: ç¼–ç¨‹ä»»åŠ¡")
    result1 = agent.chat(
        "å¸®æˆ‘åˆ†æè¿™æ®µä»£ç : def hello(): print('world')",
        session_id="session_1"
    )
    print(f"ç»“æœ: {result1}")

    # ç¤ºä¾‹ 2: è®¡ç®—ä»»åŠ¡
    print("\nğŸ”¢ ç¤ºä¾‹ 2: è®¡ç®—ä»»åŠ¡")
    result2 = agent.chat(
        "è®¡ç®— 25 * 4 + 10",
        session_id="session_2"
    )
    print(f"ç»“æœ: {result2}")

    # ç¤ºä¾‹ 3: ç ”ç©¶ä»»åŠ¡
    print("\nğŸ” ç¤ºä¾‹ 3: ç ”ç©¶ä»»åŠ¡")
    result3 = agent.chat(
        "æœç´¢å…³äº Python å¼‚æ­¥ç¼–ç¨‹çš„ä¿¡æ¯",
        session_id="session_3"
    )
    print(f"ç»“æœ: {result3}")

    # ç¤ºä¾‹ 4: æŸ¥çœ‹å¯ç”¨å·¥å…·
    print("\nğŸ› ï¸ å¯ç”¨å·¥å…·:")
    for tool_info in agent.list_tools():
        print(f"  - {tool_info['name']}: {tool_info.get('description', 'N/A')}")


if __name__ == "__main__":
    main()
